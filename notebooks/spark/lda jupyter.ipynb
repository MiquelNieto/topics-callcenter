{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Importamos las librerias necesarias.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.ml.clustering import LDA\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import re as re\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import treetaggerwrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Cargamos los datos y nos quedamos con raw_verint, todo lo que no este vacío. Eliminamos palabras cortas y Stopwords\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llamadas totales 185109\n"
     ]
    }
   ],
   "source": [
    "domo_dataset = sqlContext.read.parquet(\"dataset/domo_dataset.parquet\")\n",
    "raw_verint = domo_dataset.select(\"raw_verint\").rdd.filter(lambda x: x[\"raw_verint\"] is not None) \\\n",
    "              .map(lambda x: \" \".join(map(lambda y: \" \".join(y), x))).repartition(17)\n",
    "raw_verint.cache()\n",
    "StopWords = stopwords.words(\"spanish\")\n",
    "##Llamadas totales buenas\n",
    "print(\"llamadas totales {}\".format(raw_verint.count()))\n",
    "numeros = [\"uno\", \"dos\", \"tres\", \"cuatro\", \"cinco\", \"seis\", \"siete\", \"ocho\", \"nueve\", \"cero\"]\n",
    "tokens = raw_verint.map( lambda document: document.strip().lower()) \\\n",
    "\t.map( lambda document: re.split(\" \", document)) \\\n",
    "\t.map( lambda word: [x for x in word if x.isalpha()]) \\\n",
    "\t.map( lambda word: [x for x in word if len(x) > 3] ) \\\n",
    "\t.map( lambda word: [x for x in word if x not in StopWords]) \\\n",
    "\t.map( lambda word: [x for x in word if x not in numeros]) \\\n",
    "\t.zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Contamos el número de aparición de cada palabra. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "termCounts_tokens =  tokens.filter(lambda x: len(x)>0) \\\n",
    "    .flatMap(lambda document: document[0]) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey( lambda x,y: x + y)   \\\n",
    "    .map(lambda tuple: (tuple[1], tuple[0]))  \\\n",
    "    .sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Extraemos el vocabulario. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###umbral\n",
    "threshold_value =50 \n",
    "vocabulary_tokens = termCounts_tokens   \\\n",
    "  .filter(lambda x : x[0] < threshold_value)  \\\n",
    "  .map(lambda x: x[1])  \\\n",
    "  .zipWithIndex()  \\\n",
    "  .collectAsMap()\n",
    "vocab_size_tokens = len(vocabulary_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "creamos el dataframe necesario para aplicar LDA. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"features\", vocabSize=vocab_size_tokens, minDF=10.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "lda_df = result_cv[['index', 'features']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Aplicamos el modelo LDA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=10\n",
    "max_iterations = 50\n",
    "lda_tokens = LDA(k=num_topics, maxIter=max_iterations)\n",
    "model_tokens =lda_tokens.fit(lda_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Extraemos los topics y guardamos el modelo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model_tokens.describeTopics()\n",
    "model_tokens.save(\"model_lda_tokens.prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Imprimimos los topics. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------+\n",
      "|t1 |t2                                                                                                       |\n",
      "+---+---------------------------------------------------------------------------------------------------------+\n",
      "|0  |[rayos, adherir, campillos, cool, aplicárselo, mensualment, presionamos, oriol, nuri, ciencuenta]        |\n",
      "|1  |[campillos, cool, oriol, adherir, inversiones, bajemos, mensualment, acoso, consignado, obispo]          |\n",
      "|2  |[campillos, platicar, adherir, nuri, mensualment, penalice, seque, saludado, cool, picazón]              |\n",
      "|3  |[planteamos, enviaste, campillos, aplicárselo, adherir, informadas, desglosa, asaltado, saludado, antony]|\n",
      "|4  |[campillos, cool, adherir, ancianos, mensualment, seque, arreglaran, saludado, penalice, aplicárselo]    |\n",
      "|5  |[interpreto, saludado, campillos, desglosa, seque, cogera, solventada, aplicárselo, arreglaran, censando]|\n",
      "|6  |[campillos, adherir, cool, costaban, rayos, inversiones, acoso, mensualment, desglosa, aplicárselo]      |\n",
      "|7  |[cool, adherir, ciencuenta, campillos, desglosa, acoso, pulgada, afectaba, muestre, inversiones]         |\n",
      "|8  |[campillos, adherir, cool, mensualment, penalice, seque, obispo, acoso, saludado, allianz]               |\n",
      "|9  |[rehacer, pilo, campillos, remitirle, contestarnos, contaré, adherir, rayos, cobranzas, penalice]        |\n",
      "+---+---------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#topics.show(truncate=False)\n",
    "terms_tokens = dict([(value, key) for key, value in vocabulary_tokens.items()])\n",
    "topics_term = topics.rdd.map(lambda x: [x[0],list(map(lambda y: terms_tokens[y],x[1])), x[2]])\n",
    "topics_term.toDF([\"t1\", \"t2\"])[\"t1\", \"t2\"].show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffeeee; border-color: #ffbbbb; border-left: 5px solid #ff8888; padding: 0.5em;\">\n",
    "Parece que esto no tiene mucho sentido con los temas que queremos extraer de un call center. He probado con diferentes parámetros y nada...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Volvemos a aplicar el modelo sobre bigramas, en este caso nos quedamos solo con parejas Nombre-Nombre, Verbo Nombre y Nombre adjetivo. Y nos quedamos con la raiz. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_candidate(x):\n",
    "\tgood = [[u'VLinf', u'NC'], [u'NC', u'ADJ'], [u'NC', u'NC'] ]\n",
    "\tfor g in good:\n",
    "\t\tif (x[0][1] == g[0] and  x[1][1] == g[1]):\n",
    "\t\t\treturn True\n",
    "\treturn False\n",
    "\t\n",
    "def get_bigrams(x):\n",
    "\ttagger = treetaggerwrapper.TreeTagger(TAGLANG='es', TAGPARFILE=\"/tmp/tree/spanish.par\", TAGDIR=\"/tmp/tree/tree-tagger-3.2.1/\")\n",
    "\tpos_tags= map(lambda y: y.split(\"\\t\"),list(tagger.tag_text((\" \".join(x[0])) )))\n",
    "\tbigrams = list(nltk.bigrams(pos_tags))\n",
    "\tcandidate_bigrams = list(filter(is_candidate ,bigrams))\n",
    "\tstemmed_bigram = map(lambda x: x[0][2]  + \" \" + x[1][2],candidate_bigrams)\n",
    "\ttokens = [stemmed_bigram, x[1]]\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "def get_bigrams(x):\n",
    "\ttagger = treetaggerwrapper.TreeTagger(TAGLANG='es', TAGPARFILE=\"/tmp/tree/spanish.par\", TAGDIR=\"/tmp/tree/tree-tagger-3.2.1/\")\n",
    "\tpos_tags= list(map(lambda y: y.split(\"\\t\"),list(tagger.tag_text((\" \".join(x[0])) ))))\n",
    "\tbigrams = list(nltk.bigrams(pos_tags))\n",
    "\tcandidate_bigrams = list(filter(is_candidate ,bigrams))\n",
    "\tstemmed_bigram = list(map(lambda x: x[0][2]  + \" \" + x[1][2],candidate_bigrams))\n",
    "\ttokens = [stemmed_bigram, x[1]]\n",
    "\treturn tokens\n",
    "\n",
    "bigrams = tokens.map(get_bigrams)\t\n",
    "bigrams.cache()\n",
    "bigrams.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Contamos las ocurrencias. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "termCounts_bigrams =  bigrams.filter(lambda x: len(x)>0) \\\n",
    "    .flatMap(lambda document: document[0]) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey( lambda x,y: x + y)   \\\n",
    "    .map(lambda tuple: (tuple[1], tuple[0]))  \\\n",
    "    .sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Extraemos el vocabulario\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###umbral\n",
    "threshold_value =15 \n",
    "vocabulary_bigrams = termCounts_bigrams   \\\n",
    "  .filter(lambda x : x[0] < threshold_value)  \\\n",
    "  .map(lambda x: x[1])  \\\n",
    "  .zipWithIndex()  \\\n",
    "  .collectAsMap()\n",
    "vocab_size_bigrams = len(vocabulary_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Obtenemos el dataframe para calcular el modelo</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txts = sqlContext.createDataFrame(bigrams, [\"list_of_words\",'index'])\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"features\", vocabSize=vocab_size_bigrams, minDF=10.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n",
    "lda_df = result_cv[['index', 'features']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Aplicamos LDA</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_bigrams = LDA(k=num_topics, maxIter=max_iterations)\n",
    "model_bigrams =lda_bigrams.fit(lda_df)\n",
    "topics_bigrams = model_bigrams.describeTopics()\n",
    "model_bigrams.save(\"model_lda_bigrams.prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eeffee; border-color: #bbFFbb; border-left: 5px solid #88FF88; padding: 0.5em;\">\n",
    "Mostramos los topics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|t1 |t2                                                                                                                                                                     |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[gusto teléfono, hacer vecino, número futbol, portabilidad marido, foco champion, tipo semanal, sánchez equipo, ruido correcto, decir moviles, tiempo devolución]      |\n",
      "|1  |[mirar trámite, número futbol, ahorrar cuota, tiempo giga, emoción paso, nombre paulina, amor llamada, mes lista, tipo semanal, principio grande]                      |\n",
      "|2  |[gusto teléfono, número futbol, euro clienta, abonado caso, servicio sánchez, hacer vecino, sánchez equipo, dato regalo, ruido correcto, vez barcelona]                |\n",
      "|3  |[hacer vecino, gusto teléfono, número futbol, abonado caso, suscripción informe, banca claro, mañana mediodía, rehabilitación parte, euro clienta, portabilidad marido]|\n",
      "|4  |[hijo bono, hecho tipo, gracia detalle, descargar acuerdo, tiempo cerrado, raquel instalación, línea info, abaratar poco, intentar cliente, contraseña primera]        |\n",
      "|5  |[hacer vecino, número futbol, cheque bancario, abonado caso, momento desconexión, ayudar bien, reclamar promoción, tipo semanal, gusto teléfono, llamada puerta]       |\n",
      "|6  |[número futbol, euro clienta, hacer vecino, sánchez equipo, abonado caso, gusto teléfono, paso medio, mañana mediodía, forma pequeño, acuerdo noche]                   |\n",
      "|7  |[número futbol, promoción equipo, lado gigas, momento extraño, suscripción televisión, señora jueves, calle paralelo, problema luis, cantidad hora, mayo posible]      |\n",
      "|8  |[vez barcelona, falta permanencia, paquete eric, dato abril, disponer futbol, señor normal, gusto teléfono, decir contacto, empresa grabación, emoción paso]           |\n",
      "|9  |[hacer vecino, número futbol, gusto teléfono, cash línea, decisión casa, dirección carrer, sánchez equipo, precio regalo, vez barcelona, foco champion]                |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#topics.show(truncate=False)\n",
    "terms_bigrams = dict([(value, key) for key, value in vocabulary_bigrams.items()])\n",
    "topics_term = topics_bigrams.rdd.map(lambda x: [x[0],list(map(lambda y: terms_bigrams[y],x[1])), x[2]])\n",
    "topics_term.toDF([\"t1\", \"t2\"])[\"t1\", \"t2\"].show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffeeee; border-color: #FFbbbb; border-left: 5px solid #FF8888; padding: 0.5em;\">\n",
    "Aquí parece que al menos los bigramas tienen algo más de sentido con una empresa telefónica, sin embargo no soy capaz de distinguir temas concretos y en algunos casos como el 9 parece que hablan de Barcelona (carrer, Barcelona). No sé como podemos forzar al algoritmo a descubrir los temas que nos interesan.  ¿Alguna idea?\n",
    "    <BR><BR>\n",
    "He probado con otros números de topics, pero los resultados no han sido mejores. \n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
